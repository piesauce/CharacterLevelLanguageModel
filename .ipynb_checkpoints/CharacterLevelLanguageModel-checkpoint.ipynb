{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook, we will attempt to generate reasonable sounding dinosaur names using a character level langauge model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is contained in the file 'dinos.txt' and is comprised of 1536 dinosaur names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19909 27\n"
     ]
    }
   ],
   "source": [
    "train_data = open('dinos.txt', 'r').read().lower()\n",
    "alphabet = list(set(train_data))      # Retrieve list of characters\n",
    "doc_size = len(train_data)            # Calculate training data size\n",
    "alphabet_size = len(alphabet)\n",
    "print(doc_size, alphabet_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create two dictionaries - char_dict maps an index to a character and inverse_char_dict maps a character to its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'} {'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_dict = {i:char for i, char in enumerate(sorted(alphabet))}\n",
    "inverse_char_dict = {char:i for i, char in enumerate(sorted(alphabet))}\n",
    "print(char_dict, inverse_char_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that performs gradient clipping in order to prevent exploding gradients. We accomplish this using the numpy.clip function. https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_val):\n",
    "    \n",
    "    dwax = gradients['dwax']\n",
    "    dwaa = gradients['dwaa']\n",
    "    dwya = gradients['dwya']\n",
    "    dba = gradients['dba']\n",
    "    dby = gradients['dby']\n",
    "    \n",
    "    for grad in [dwax, dwaa, dwya, dba, dby]:\n",
    "        np.clip(grad, a_min = -max_val, a_max = max_val, out=grad)\n",
    "    \n",
    "    gradients = {'dwax': dwax, 'dwaa': dwaa, 'dwya': dwya, 'dba': dba, 'dby': dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sample(params, inverse_char_dict, seed):\n",
    "    \"\"\"\n",
    "    Generate a sequence of characters according to the probability distribution output from the RNN.\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing the parameters wax, waa, wya, ba, by.\n",
    "    inverse_char_dict -- dictionary mapping each character to its index\n",
    "    seed -- seed for the randomizer\n",
    "\n",
    "    Returns:\n",
    "    indices -- list containing the indices of the sampled characters\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = []\n",
    "    cur_idx = -1\n",
    "    count = 0\n",
    "    \n",
    "    wax = params['wax']\n",
    "    waa = params['waa']\n",
    "    wya = params['wya']\n",
    "    ba = params['ba']\n",
    "    by = params['by']\n",
    "    \n",
    "    alphabet_size = by.shape[0]\n",
    "    act_size = waa.shape[1]\n",
    "    \n",
    "    X = np.zeros((alphabet_size, 1))\n",
    "    a = np.zeros((act_size, 1))\n",
    "    \n",
    "    newline_idx = inverse_char_dict['\\n']\n",
    "    \n",
    "    while(cur_idx != newline_idx and count < 50):\n",
    "        \n",
    "        a_new = np.tanh((np.matmul(wax, X) + np.matmul(waa, a) + ba))\n",
    "        y = np.matmul(wya, a_new) + by\n",
    "        expn = np.exp(y - np.max(y))\n",
    "        y_predict = expn / expn.sum(axis=0)\n",
    "        \n",
    "        np.random.seed(count+seed)\n",
    "        \n",
    "        idx_list = list(inverse_char_dict.values())\n",
    "        cur_idx = np.random.choice(idx_list, p = y_predict.flatten())\n",
    "        indices.append(cur_idx)\n",
    "        \n",
    "        X = np.zeros((alphabet_size, 1))\n",
    "        X[cur_idx] = 1\n",
    "        a = a_new\n",
    "        seed +=1\n",
    "        count+=1 \n",
    "    if(count == 50):\n",
    "        indices.append(inverse_char_dict['\\n'])\n",
    "        \n",
    "    return indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward_step(x_t, a, params):\n",
    "    \"\"\"\n",
    "    Run one step of the forward progation at time step t\n",
    "    \n",
    "    Arguments:\n",
    "    x_t -- input vector at time step t\n",
    "    a -- activation from the previous time step\n",
    "    params -- dictionary containing the parameters wax, waa, wya, ba, by\n",
    "    Returns:\n",
    "    a_new -- activation from the current time step\n",
    "    y_predict_t -- output prediction from the current time step\n",
    "    \"\"\"\n",
    "    wax = params['wax']\n",
    "    waa = params['waa']\n",
    "    wya = params['wya']\n",
    "    ba = params['ba']\n",
    "    by = params['by']\n",
    "    \n",
    "    a_new = np.tanh(np.matmul(wax, x_t) + np.matmul(waa, a) + ba)\n",
    "    \n",
    "    y = np.matmul(wya, a_new) + by\n",
    "    expn = np.exp(y - np.max(y))\n",
    "    y_predict_t = expn / expn.sum(axis=0)     #Compute softmax\n",
    "    \n",
    "    return a_new, y_predict_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, params, alphabet_size=27):\n",
    "    \"\"\"\n",
    "    Run forward propagation through the RNN\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input vector containing character indices\n",
    "    Y -- same as X, but shifted one position to the left\n",
    "    a0 -- activation at time step 0\n",
    "    params -- dictionary containing the parameters wax, waa, wya, ba, by\n",
    "    alphabet_size -- number of unique characters\n",
    "    Returns:\n",
    "    loss -- cross entropy loss\n",
    "    cache --  cache to be used during backpropagation\n",
    "    \"\"\"\n",
    "    x = {}\n",
    "    a = {}\n",
    "    y_predict = {}\n",
    "    loss = 0\n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        x[t] = np.zeros((alphabet_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        a[t], y_predict[t] = rnn_forward_step(x[t], a[t-1], params)\n",
    "        loss -= np.log(y_predict[t][Y[t],0])\n",
    "    cache = (y_predict, a, x)\n",
    "    \n",
    "    return loss, cache     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward_step(params, gradients, x, a, a_new, dy):\n",
    "    \"\"\"\n",
    "    Run one step of backpropagation at time step t\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing the parameters wax, waa, wya, ba, by\n",
    "    gradients -- dictionary containing the gradients dwya, dby, dba, dwax, dwaa, da\n",
    "    x -- input vector at time step t\n",
    "    a -- activation at time step t-1\n",
    "    a_new -- activation at time step t\n",
    "    dy -- initial gradient\n",
    "    Returns:\n",
    "    gradients -- dictionary containing the gradients dwya, dby, dba, dwax, dwaa, da\n",
    "    \"\"\"\n",
    "    gradients['dwya'] += np.dot(dy, a_new.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(params['wya'].T, dy) + gradients['da']\n",
    "    dat = (1 - a_new * a_new) * da\n",
    "    gradients['dba'] += dat\n",
    "    gradients['dwax'] += np.dot(dat, x.T)\n",
    "    gradients['dwaa'] += np.dot(dat, a.T)\n",
    "    gradients['da'] = np.dot(params['waa'].T, dat)\n",
    "    \n",
    "    return gradients    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_backward(X, y, params, cache):\n",
    "    \"\"\"\n",
    "    Run backpropagation through the network\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input vector containing character indices\n",
    "    y -- same as X, but shifted one position to the left\n",
    "    params -- dictionary containing the parameters wax, waa, wya, ba, by\n",
    "    cache --  cache to be used during backpropagation\n",
    "    Returns:\n",
    "    gradients -- dictionary containing the gradients dwya, dby, dba, dwax, dwaa, da\n",
    "    a -- final activation of the RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    gradients = {}\n",
    "    \n",
    "    wya = params['wya']\n",
    "    waa = params['waa']\n",
    "    wax = params['wax']\n",
    "    by = params['by']\n",
    "    ba = params['ba']\n",
    "    \n",
    "    (y_predict, a, x) = cache\n",
    "    \n",
    "    gradients['dwya'] = np.zeros_like(wya)\n",
    "    gradients['dwaa'] = np.zeros_like(waa)\n",
    "    gradients['dwax'] = np.zeros_like(wax)\n",
    "    gradients['dby'] = np.zeros_like(by)\n",
    "    gradients['dba'] = np.zeros_like(ba)\n",
    "    gradients['da'] = np.zeros_like(a[0])\n",
    "    \n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_predict[t])\n",
    "        dy[y[t]] -= 1\n",
    "        gradients = rnn_backward_step(params, gradients, x[t], a[t-1], a[t], dy)\n",
    "        \n",
    "    return gradients, a   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_params(params, gradients, learning_rate):\n",
    "    \n",
    "    params['wya'] -= learning_rate * gradients['dwya']\n",
    "    params['waa'] -= learning_rate * gradients['dwaa']\n",
    "    params['wax'] -= learning_rate * gradients['dwax']\n",
    "    params['by'] -= learning_rate * gradients['dby']\n",
    "    params['ba'] -= learning_rate * gradients['dba']\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, y, a, params, learning_rate):\n",
    "    \n",
    "    loss, cache = rnn_forward(X, y, a, params)\n",
    "    gradients, a = rnn_backward(X, y, params, cache)\n",
    "    gradients = clip_gradients(gradients, 5)\n",
    "    params = update_params(params, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(train_data, char_dict, inverse_char_dict, alphabet_size, num_samples, h_size, num_iterations):\n",
    "    \n",
    "    params = {}\n",
    "    in_size = alphabet_size\n",
    "    ou_size = alphabet_size\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    params['wya'] = np.random.randn(ou_size, h_size) * 0.01 \n",
    "    params['waa'] = np.random.randn(h_size, h_size) * 0.01 \n",
    "    params['wax'] = np.random.randn(h_size, in_size) * 0.01 \n",
    "    params['by'] = np.zeros((ou_size, 1)) \n",
    "    params['ba'] = np.zeros((h_size, 1))\n",
    "    \n",
    "    a = np.zeros((h_size, 1))\n",
    "    \n",
    "    loss = -np.log(1.0/alphabet_size) * num_samples\n",
    "    \n",
    "    with open('dinos.txt') as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        cur_idx = i % len(examples)\n",
    "        X = [None] + [inverse_char_dict[char] for char in examples[cur_idx]]\n",
    "        Y = X[1:] + [inverse_char_dict['\\n']]\n",
    "        \n",
    "        cur_loss, gradients, a = optimize(X, Y, a, params, learning_rate=0.01)\n",
    "        loss = loss * 0.999 + cur_loss * 0.001\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            \n",
    "            print('Loss after iteration %d : %f' % (i, loss))\n",
    "            \n",
    "            seed = 0\n",
    "            \n",
    "            for j in range(num_samples):\n",
    "                \n",
    "                indices =  gen_sample(params, inverse_char_dict, seed)\n",
    "                name = ''.join(char_dict[idx] for idx in indices)\n",
    "                print('%s' % (name))\n",
    "                \n",
    "                seed += 1\n",
    "                print('\\n')\n",
    "    \n",
    "    return params        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 23.087338\n",
      "nkzxwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "\n",
      "kneb\n",
      "\n",
      "\n",
      "\n",
      "kzxwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "\n",
      "neb\n",
      "\n",
      "\n",
      "\n",
      "zxwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "\n",
      "eb\n",
      "\n",
      "\n",
      "\n",
      "xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 2000 : 27.913822\n",
      "liustolldoravgpsrarinhsianthymechalujdhangaloltonp\n",
      "\n",
      "\n",
      "\n",
      "hmcaberteecltksatotleeycertactorapherosaurus\n",
      "\n",
      "\n",
      "\n",
      "hytrpclfppeurusacroresianusvihandes\n",
      "\n",
      "\n",
      "\n",
      "lecalpsamanthrgdjus\n",
      "\n",
      "\n",
      "\n",
      "xusjciloraurus\n",
      "\n",
      "\n",
      "\n",
      "acalpsalanthrfenwneeycerun\n",
      "\n",
      "\n",
      "\n",
      "troligoraurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 4000 : 25.842094\n",
      "liusisaurus\n",
      "\n",
      "\n",
      "\n",
      "inga\n",
      "\n",
      "\n",
      "\n",
      "iusosaurus\n",
      "\n",
      "\n",
      "\n",
      "madalosaurus\n",
      "\n",
      "\n",
      "\n",
      "xuskchgosaurus\n",
      "\n",
      "\n",
      "\n",
      "baagosaurus\n",
      "\n",
      "\n",
      "\n",
      "tosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 6000 : 24.572623\n",
      "onyuselmeroptis\n",
      "\n",
      "\n",
      "\n",
      "kigcalosaurus\n",
      "\n",
      "\n",
      "\n",
      "lytromonosaurus\n",
      "\n",
      "\n",
      "\n",
      "olealosaurus\n",
      "\n",
      "\n",
      "\n",
      "wussaurus\n",
      "\n",
      "\n",
      "\n",
      "eeaisope\n",
      "\n",
      "\n",
      "\n",
      "tosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 8000 : 24.087631\n",
      "onxusberatons\n",
      "\n",
      "\n",
      "\n",
      "loma\n",
      "\n",
      "\n",
      "\n",
      "lytrodon\n",
      "\n",
      "\n",
      "\n",
      "olaadosaurus\n",
      "\n",
      "\n",
      "\n",
      "wussaurus\n",
      "\n",
      "\n",
      "\n",
      "eeahron\n",
      "\n",
      "\n",
      "\n",
      "trodon\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 10000 : 23.862407\n",
      "nivusaurus\n",
      "\n",
      "\n",
      "\n",
      "jnecaisaurosaurus\n",
      "\n",
      "\n",
      "\n",
      "lustreodon\n",
      "\n",
      "\n",
      "\n",
      "nea\n",
      "\n",
      "\n",
      "\n",
      "vuslanesaurus\n",
      "\n",
      "\n",
      "\n",
      "daadosaurus\n",
      "\n",
      "\n",
      "\n",
      "trocherogurosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 12000 : 23.261131\n",
      "onyxsolnesaurus\n",
      "\n",
      "\n",
      "\n",
      "kicacerhaechunsaurus\n",
      "\n",
      "\n",
      "\n",
      "lustolonnoouosaurus\n",
      "\n",
      "\n",
      "\n",
      "ola\n",
      "\n",
      "\n",
      "\n",
      "vutolnesaurus\n",
      "\n",
      "\n",
      "\n",
      "edalosaurus\n",
      "\n",
      "\n",
      "\n",
      "tosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 14000 : 23.264572\n",
      "novosaurus\n",
      "\n",
      "\n",
      "\n",
      "kolaaestekanorsaurus\n",
      "\n",
      "\n",
      "\n",
      "lustrionor\n",
      "\n",
      "\n",
      "\n",
      "oldagtor\n",
      "\n",
      "\n",
      "\n",
      "vusteolophus\n",
      "\n",
      "\n",
      "\n",
      "eeaeptor\n",
      "\n",
      "\n",
      "\n",
      "troconosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 16000 : 23.217015\n",
      "levusdandos\n",
      "\n",
      "\n",
      "\n",
      "hiacalptor\n",
      "\n",
      "\n",
      "\n",
      "iusosaurus\n",
      "\n",
      "\n",
      "\n",
      "lacakosaurus\n",
      "\n",
      "\n",
      "\n",
      "vuspamespasaurus\n",
      "\n",
      "\n",
      "\n",
      "ca\n",
      "\n",
      "\n",
      "\n",
      "tos\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 18000 : 22.895919\n",
      "oryxnoriacorus\n",
      "\n",
      "\n",
      "\n",
      "lone\n",
      "\n",
      "\n",
      "\n",
      "lytrornathesaurus\n",
      "\n",
      "\n",
      "\n",
      "opa\n",
      "\n",
      "\n",
      "\n",
      "uspsaurus\n",
      "\n",
      "\n",
      "\n",
      "egalosaurus\n",
      "\n",
      "\n",
      "\n",
      "trophesaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 20000 : 22.939575\n",
      "mixtrochenkhylosaurus\n",
      "\n",
      "\n",
      "\n",
      "lola\n",
      "\n",
      "\n",
      "\n",
      "lutosaurus\n",
      "\n",
      "\n",
      "\n",
      "madaironabhuluhanthacumasocetor\n",
      "\n",
      "\n",
      "\n",
      "vustephosaurus\n",
      "\n",
      "\n",
      "\n",
      "gabasilachumuhanthacumasocescomalernxllirnkcomorhi\n",
      "\n",
      "\n",
      "\n",
      "trocheosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 22000 : 22.830153\n",
      "nixtrocerator\n",
      "\n",
      "\n",
      "\n",
      "lkdeacosaurus\n",
      "\n",
      "\n",
      "\n",
      "lustohiceratops\n",
      "\n",
      "\n",
      "\n",
      "necaerorachus\n",
      "\n",
      "\n",
      "\n",
      "usisaurus\n",
      "\n",
      "\n",
      "\n",
      "eg\n",
      "\n",
      "\n",
      "\n",
      "trocenosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 24000 : 22.810886\n",
      "liusosaurus\n",
      "\n",
      "\n",
      "\n",
      "hodaaesceia\n",
      "\n",
      "\n",
      "\n",
      "hyxus\n",
      "\n",
      "\n",
      "\n",
      "leaaeskeianseurintha\n",
      "\n",
      "\n",
      "\n",
      "uspraonosaurus\n",
      "\n",
      "\n",
      "\n",
      "daachyedantetha\n",
      "\n",
      "\n",
      "\n",
      "trocenatosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 26000 : 22.672382\n",
      "opustonlipiiumose\n",
      "\n",
      "\n",
      "\n",
      "liceeitromaptitenaurus\n",
      "\n",
      "\n",
      "\n",
      "lutrodonophus\n",
      "\n",
      "\n",
      "\n",
      "olcakrona\n",
      "\n",
      "\n",
      "\n",
      "ustrengoraus\n",
      "\n",
      "\n",
      "\n",
      "egacosaurus\n",
      "\n",
      "\n",
      "\n",
      "troceratops\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 28000 : 22.751410\n",
      "nlyusaniciomtasaurus\n",
      "\n",
      "\n",
      "\n",
      "kicabascalansaurus\n",
      "\n",
      "\n",
      "\n",
      "kutrokhanicopsdyangimis\n",
      "\n",
      "\n",
      "\n",
      "nia\n",
      "\n",
      "\n",
      "\n",
      "voticephelus\n",
      "\n",
      "\n",
      "\n",
      "ehaesdelanqisaurus\n",
      "\n",
      "\n",
      "\n",
      "trocerator\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 30000 : 22.669220\n",
      "mayusolongmaverntaurus\n",
      "\n",
      "\n",
      "\n",
      "ipacantrodasthulirus\n",
      "\n",
      "\n",
      "\n",
      "ivuskarasthvesaurus\n",
      "\n",
      "\n",
      "\n",
      "macaesig\n",
      "\n",
      "\n",
      "\n",
      "ustriddosaurus\n",
      "\n",
      "\n",
      "\n",
      "edaesnia\n",
      "\n",
      "\n",
      "\n",
      "trocheronus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 32000 : 22.329924\n",
      "maxusianasaurus\n",
      "\n",
      "\n",
      "\n",
      "inacalosaurus\n",
      "\n",
      "\n",
      "\n",
      "jususaurus\n",
      "\n",
      "\n",
      "\n",
      "macaisaurus\n",
      "\n",
      "\n",
      "\n",
      "vustarasaurus\n",
      "\n",
      "\n",
      "\n",
      "efaernadanthus\n",
      "\n",
      "\n",
      "\n",
      "trocheosaurus\n",
      "\n",
      "\n",
      "\n",
      "Loss after iteration 34000 : 22.529164\n",
      "maytosaurus\n",
      "\n",
      "\n",
      "\n",
      "ima\n",
      "\n",
      "\n",
      "\n",
      "jussidongmaveptodon\n",
      "\n",
      "\n",
      "\n",
      "maca\n",
      "\n",
      "\n",
      "\n",
      "wrosaurus\n",
      "\n",
      "\n",
      "\n",
      "ea\n",
      "\n",
      "\n",
      "\n",
      "trohakosaurus\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = model(train_data, char_dict, inverse_char_dict, alphabet_size=27, num_samples=7, h_size=50, num_iterations=35000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After 35,000 iterations, we are able to generate reasonably dinosaur-y sounding names like 'trohakosaurus', 'vustarasaurus', and 'maytosaurus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
